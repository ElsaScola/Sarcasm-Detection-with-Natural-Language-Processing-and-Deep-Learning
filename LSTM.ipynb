{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "notify_time": "5",
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "510.85px",
        "left": "1550px",
        "right": "20px",
        "top": "120px",
        "width": "350px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    },
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqvvBLQkx3-q",
        "colab_type": "text"
      },
      "source": [
        "## LSTM with validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7yHquTOx3-r",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-10T08:38:45.362099Z",
          "start_time": "2020-05-10T08:38:41.921456Z"
        },
        "id": "upfST-E3x3-t",
        "colab_type": "code",
        "outputId": "f94b91f9-ff6e-415c-fe56-8629ec7bf094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model, Sequential\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.layers import LSTM\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIyMIwvVx3-3",
        "colab_type": "text"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOpyIc-fymIH",
        "colab_type": "code",
        "outputId": "ea125c6a-4f7a-4109-edd0-2e69b4b6caaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-10T08:38:45.544234Z",
          "start_time": "2020-05-10T08:38:45.443796Z"
        },
        "id": "-RuMjGnAx3-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/My Drive/TFMColab/train.csv\")\n",
        "val = pd.read_csv(\"/content/drive/My Drive/TFMColab/val.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/TFMColab/test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuhwIq9rx3--",
        "colab_type": "text"
      },
      "source": [
        "### Tokenize\n",
        "Fit the tokenizer only on the text of training data.\n",
        "Then, we use that same tokenizer to transform the texts of train, val and test sets to sequences of integers.\n",
        "\n",
        "It's possible to fit on the entire data. But it's probably a better idea to reserve a token for \"unknown\" words (oov_token=True), for the cases when you find new test data with words your model has never seen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9KhphwGx3-_",
        "colab_type": "text"
      },
      "source": [
        "### Tokenize\n",
        "Fit the tokenizer only on the text of training data.\n",
        "Then, we use that same tokenizer to transform the texts of train, val and test sets to sequences of integers.\n",
        "\n",
        "It's possible to fit on the entire data. But it's probably a better idea to reserve a token for \"unknown\" words (oov_token=True), for the cases when you find new test data with words your model has never seen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-10T08:38:47.348652Z",
          "start_time": "2020-05-10T08:38:45.724660Z"
        },
        "id": "vyi3UEMhx3-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_features = 10000 # max num words\n",
        "maxlen = 25 \n",
        "embedding_size = 200\n",
        "\n",
        "# create the tokenizer with the maximum number of words to keep, \n",
        "# based on word frequency. \n",
        "# Only the most common num_words-1 words will be kept.\n",
        "tokenizer = Tokenizer(num_words=max_features, oov_token = True)\n",
        "\n",
        "# fit the tokenizer on the headlines\n",
        "tokenizer.fit_on_texts(list(train['headline']))\n",
        "\n",
        "# Transforms each text in texts to a sequence of integers.\n",
        "train_X = tokenizer.texts_to_sequences(train['headline'])\n",
        "test_X = tokenizer.texts_to_sequences(test['headline'])\n",
        "val_X = tokenizer.texts_to_sequences(val['headline'])\n",
        "\n",
        "# transforms a list of num_samples sequences (lists of integers)\n",
        "# into a 2D Numpy array of shape (num_samples, num_timesteps).\n",
        "train_X = pad_sequences(train_X, maxlen = maxlen)\n",
        "test_X = pad_sequences(test_X, maxlen = maxlen)\n",
        "val_X = pad_sequences(val_X, maxlen = maxlen)\n",
        "\n",
        "train_y = train['is_sarcastic']\n",
        "test_y = test['is_sarcastic']\n",
        "val_y = val['is_sarcastic']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPffAJDIx3_E",
        "colab_type": "text"
      },
      "source": [
        "Load glove embedding set, construct embedding matrix for words in word_index:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-10T08:39:30.132017Z",
          "start_time": "2020-05-10T08:38:47.536800Z"
        },
        "id": "Y9tT0SZbx3_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "13a4fa9e-6402-4adb-fd78-b3a62e9239ca"
      },
      "source": [
        "# load embeddings\n",
        "EMBEDDING_FILE = '/content/drive/My Drive/TFMColab/glove.6B.200d.txt'\n",
        "\n",
        "def get_coefs(word,*arr): \n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "# first, build index mapping words in the embeddings set\n",
        "# to their embedding vector\n",
        "embeddings_index = {}\n",
        "with open(EMBEDDING_FILE, encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        word, coefs = get_coefs(*line.split(\" \"))\n",
        "        embeddings_index[word] = coefs\n",
        "            \n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "\n",
        "# Random embedding vector for unknown words.\n",
        "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n",
        "# prepare embedding matrix\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: \n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: \n",
        "        # words not found in embedding index will be random\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  if self.run_code(code, result):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LZe5Ntpx3_K",
        "colab_type": "text"
      },
      "source": [
        "### LSTM Model\n",
        "Model Parameters:\n",
        "\n",
        "- **Activation Function**: I have used ReLU as the activation function. ReLU is a non-linear activation function, which helps complex relationships in the data to be captured by the model.\n",
        "\n",
        "- **Optimiser**: We use adam optimiser, which is an adaptive learning rate optimiser.\n",
        "\n",
        "- **Loss function**: We will train a network to output a probability over the 2 classes using Sigmoid Loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-10T08:48:19.482797Z",
          "start_time": "2020-05-10T08:38:46.714Z"
        },
        "id": "ykFP7yQcx3_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model structure\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_size, weights = [embedding_matrix]))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(40, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.TruePositives()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysvBfLhAx3_Q",
        "colab_type": "text"
      },
      "source": [
        "### Save the best model and early stopping\n",
        "To prevent the model from overfitting I have enabled early stopping.\n",
        "\n",
        "Early stopping is a method that allows us to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out/validation dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-10T08:48:19.494511Z",
          "start_time": "2020-05-10T08:38:58.047Z"
        },
        "id": "kP9f8-oNx3_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model after every epoch.\n",
        "saveBestModel = keras.callbacks.ModelCheckpoint('/content/drive/My Drive/TFMColab/best_model.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "# Stop training when a monitored quantity has stopped improving.\n",
        "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DePKu3MZx3_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "17251694-30bf-40fc-e413-be155c49d1db"
      },
      "source": [
        "# Fit the model\n",
        "batch_size = 100\n",
        "epochs = 25\n",
        "model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs, validation_data=(val_X, val_y), callbacks=[saveBestModel, earlyStopping])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 19952 samples, validate on 2850 samples\n",
            "Epoch 1/25\n",
            "19952/19952 [==============================] - 21s 1ms/step - loss: 0.6235 - accuracy: 0.6261 - precision_1: 0.6191 - recall_1: 0.5576 - true_positives_1: 5296.0000 - val_loss: 0.4009 - val_accuracy: 0.8260 - val_precision_1: 0.8467 - val_recall_1: 0.7697 - val_true_positives_1: 1033.0000\n",
            "Epoch 2/25\n",
            "  100/19952 [..............................] - ETA: 22s - loss: 0.3566 - accuracy: 0.8300 - precision_1: 0.8750 - recall_1: 0.7447 - true_positives_1: 35.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "19952/19952 [==============================] - 18s 903us/step - loss: 0.3575 - accuracy: 0.8583 - precision_1: 0.8577 - recall_1: 0.8420 - true_positives_1: 7997.0000 - val_loss: 0.3279 - val_accuracy: 0.8600 - val_precision_1: 0.8516 - val_recall_1: 0.8510 - val_true_positives_1: 1142.0000\n",
            "Epoch 3/25\n",
            "19952/19952 [==============================] - 18s 899us/step - loss: 0.2398 - accuracy: 0.9156 - precision_1: 0.9125 - recall_1: 0.9101 - true_positives_1: 8644.0000 - val_loss: 0.3387 - val_accuracy: 0.8670 - val_precision_1: 0.8707 - val_recall_1: 0.8428 - val_true_positives_1: 1131.0000\n",
            "Epoch 4/25\n",
            "19952/19952 [==============================] - 18s 899us/step - loss: 0.1608 - accuracy: 0.9456 - precision_1: 0.9437 - recall_1: 0.9419 - true_positives_1: 8946.0000 - val_loss: 0.3656 - val_accuracy: 0.8670 - val_precision_1: 0.8305 - val_recall_1: 0.9016 - val_true_positives_1: 1210.0000\n",
            "Epoch 5/25\n",
            "19952/19952 [==============================] - 18s 910us/step - loss: 0.1058 - accuracy: 0.9660 - precision_1: 0.9634 - recall_1: 0.9653 - true_positives_1: 9168.0000 - val_loss: 0.4866 - val_accuracy: 0.8653 - val_precision_1: 0.8456 - val_recall_1: 0.8733 - val_true_positives_1: 1172.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f9101633d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWBKsDHBx3_Y",
        "colab_type": "text"
      },
      "source": [
        "Stops in the Epoch 6 out of 25, this is thanks to the validation set, that prevents us to overfit the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Moc3Pr8Ox3_Z",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate model results with test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k9Dap4Ix3_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "57e8f30e-e328-4267-a6d4-8b24a8d430fd"
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy', 'precision_1', 'recall_1', 'true_positives_1']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FcLBrBTx3_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4e72e755-bfe6-4c3f-e153-e73bbd1f8825"
      },
      "source": [
        "loss, accuracy, precision, recall, true_positives = model.evaluate(test_X, test_y, batch_size=batch_size)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5701/5701 [==============================] - 1s 88us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFLlzmcdx3_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mult_pr=precision*recall\n",
        "sum_pr=precision+recall\n",
        "div=mult_pr/sum_pr\n",
        "f1_score=2*div"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjAtnmCPx3_m",
        "colab_type": "text"
      },
      "source": [
        "#### Loss, Accuracy, Precision, Recall and F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75OjSwarx3_m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "8873cbdd-6f3f-464a-f49e-a0f59c1eecf1"
      },
      "source": [
        "print('Loss:',loss)\n",
        "print('Accuracy:',accuracy)\n",
        "print('Precision:',precision)\n",
        "print('Recall:',recall)\n",
        "print('f1 score:',f1_score)\n",
        "print('True positives:',true_positives)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.4391108504660693\n",
            "Accuracy: 0.8680933117866516\n",
            "Precision: 0.8561046719551086\n",
            "Recall: 0.8687315583229065\n",
            "f1 score: 0.8623718967628665\n",
            "True positives: 2356.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udKLHyRYx3_r",
        "colab_type": "text"
      },
      "source": [
        "# Extract FalsePositives and FalseNegatives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ij8tIfwx3_r",
        "colab_type": "text"
      },
      "source": [
        "We first get the list of predictions. In the confusion matrix it can be observed that the number of True Positives is the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K04N8gmx3_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_y = model.predict_classes(test_X, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MeXIJd2Jx3_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2a9d301e-f99a-4ee9-9e42-9ba387794857"
      },
      "source": [
        "confusion_matrix(test_y, pred_y)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2593,  396],\n",
              "       [ 356, 2356]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY1Z798ix3_0",
        "colab_type": "text"
      },
      "source": [
        "We build a function to compare the predicted values to the actual values and extract the FalsePositives and FalseNegatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLK96ycjx3_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getFP_FN_lists(test_X, test_y, pred_y):\n",
        "    FP_text = []\n",
        "    FP_index = []\n",
        "    FN_text = []\n",
        "    FN_index = []\n",
        "    for i in range(len(test_y)):\n",
        "        if(pred_y[i]==1 and test_y[test_y.index[i]]==0):\n",
        "            FP_text.append(test['headline'][test_y.index[i]])\n",
        "            FP_index.append(test_y.index[i])\n",
        "        elif(pred_y[i]==0 and test_y[test_y.index[i]]==1):\n",
        "            FN_text.append(test['headline'][test_y.index[i]])\n",
        "            FN_index.append(test_y.index[i])\n",
        "            \n",
        "    return FP_text,FP_index,FN_text,FN_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wsKoeEdx3_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Returns 2 dataframes, one with all the False Positives and one with all the False Negatives'''\n",
        "def getFP_FN(test_X, test_y, pred_y):\n",
        "    FP_text,FP_index,FN_text,FN_index = getFP_FN_lists(test_X, test_y, pred_y)\n",
        "    d_FP = {'FP_text':FP_text,'FP_index':FP_index}\n",
        "    df_FP = pd.DataFrame(d_FP)\n",
        "    d_FN = {'FN_text':FN_text,'FN_index':FN_index}\n",
        "    df_FN = pd.DataFrame(d_FN)\n",
        "    \n",
        "    return df_FP,df_FN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHh9434Ex3_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We get the FPs and FNs as DataFrames and store them to CSVs\n",
        "df_FP,df_FN = getFP_FN(test_X, test_y, pred_y)\n",
        "df_FP.to_csv('FP.csv', index=True)\n",
        "df_FN.to_csv('FN.csv', index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kQ7XdBJ2Xwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}