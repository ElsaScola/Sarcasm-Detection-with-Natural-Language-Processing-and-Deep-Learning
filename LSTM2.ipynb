{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "import h5py\n",
    "import utility_functions as uf\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55328\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...             0\n",
       "1  the 'roseanne' revival catches up to our thorn...             0\n",
       "2  mom starting to fear son's web series closest ...             1\n",
       "3  boehner just wants wife to listen, not come up...             1\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_json(\"./data/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "df2 = pd.read_json(\"./data/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\n",
    "# re-order attibute columns in df2\n",
    "df2 = df2[['article_link','headline','is_sarcastic']]\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df = df.drop(['article_link'], axis=1)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_all(data_dir, all_data_path,pred_path, gloveFile, first_run, load_all):\n",
    "\n",
    "    # Load embeddings for the filtered glove list\n",
    "    if load_all == True:\n",
    "        weight_matrix, word_idx = uf.load_embeddings(gloveFile)\n",
    "    else:\n",
    "        weight_matrix, word_idx = uf.load_embeddings(filtered_glove_path)\n",
    "\n",
    "    len(word_idx)\n",
    "    len(weight_matrix)\n",
    "\n",
    "    #%%\n",
    "    # create test, validation and trainng data\n",
    "    all_data = uf.read_data(all_data_path)\n",
    "    train_data, test_data, dev_data = uf.training_data_split(all_data, 0.8, data_dir)\n",
    "\n",
    "    train_data = train_data.reset_index()\n",
    "    dev_data = dev_data.reset_index()\n",
    "    test_data = test_data.reset_index()\n",
    "\n",
    "    #%%\n",
    "    # inputs from dl_sentiment that are hard coded but need to be automated\n",
    "    maxSeqLength, avg_words, sequence_length = uf.maxSeqLen(all_data)\n",
    "    numClasses = 10\n",
    "    #%%\n",
    "\n",
    "     # load Training data matrix\n",
    "    train_x = uf.tf_data_pipeline_nltk(train_data, word_idx, weight_matrix, maxSeqLength)\n",
    "    test_x = uf.tf_data_pipeline_nltk(test_data, word_idx, weight_matrix, maxSeqLength)\n",
    "    val_x = uf.tf_data_pipeline_nltk(dev_data, word_idx, weight_matrix, maxSeqLength)\n",
    "\n",
    "    #%%\n",
    "    # load labels data matrix\n",
    "    train_y = uf.labels_matrix(train_data)\n",
    "    val_y = uf.labels_matrix(dev_data)\n",
    "    test_y = uf.labels_matrix(test_data)\n",
    "\n",
    "\n",
    "     #%%\n",
    "\n",
    "    # summarize size\n",
    "    print(\"Training data: \")\n",
    "    print(train_x.shape)\n",
    "    print(train_y.shape)\n",
    "\n",
    "    # Summarize number of classes\n",
    "    print(\"Classes: \")\n",
    "    print(np.unique(train_y.shape[1]))\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y, weight_matrix, word_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    max_words = 56 # max no of words in your training data\n",
    "    batch_size = 2000 # batch size for training\n",
    "    EMBEDDING_DIM = 100 # size of the word embeddings\n",
    "    train_flag = True # set True if in training mode else False if in prediction mode\n",
    "\n",
    "    if train_flag:\n",
    "        # create training, validation and test data sets\n",
    "        # load the dataset\n",
    "        path = '.'\n",
    "        data_dir = path+'/Data'\n",
    "        all_data_path = path+'/Data/'\n",
    "        pred_path = path+'/Data/output_model/test_pred.csv'\n",
    "        gloveFile = path+'/embeddings/glove_6B_100d.txt'\n",
    "        first_run = False\n",
    "        load_all = True\n",
    "        \n",
    "        # Split the data into train, val and test sets. Also the corresponding embeddings for the data are stored in the weight_matrix variable.\n",
    "        train_x, train_y, test_x, test_y, val_x, val_y, weight_matrix, word_idx = load_data_all(data_dir, all_data_path,pred_path, gloveFile, first_run, load_all)\n",
    "        \n",
    "        # create model strucutre\n",
    "        model = create_model_rnn(weight_matrix, max_words, EMBEDDING_DIM)\n",
    "\n",
    "        # train the model\n",
    "        trained_model =train_model(model,train_x, train_y, test_x, test_y, val_x, val_y, batch_size, path)   # run model live\n",
    "\n",
    "\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(path+\"/model/best_model.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "\n",
    "    else:\n",
    "        gloveFile = path+'/Data/glove/glove_6B_100d.txt'\n",
    "        weight_matrix, word_idx = uf.load_embeddings(gloveFile)\n",
    "        weight_path = path +'/model/best_weights_bi_glove.hdf5'\n",
    "        loaded_model = load_model(weight_path)\n",
    "        loaded_model.summary()\n",
    "        data_sample = \"Great!! it is raining today!!\"\n",
    "        result = live_test(loaded_model,data_sample, word_idx)\n",
    "        print (result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
